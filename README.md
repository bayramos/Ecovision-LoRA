EcoVision-LoRA

Predicting Urban Economic Intensity from High-Resolution Satellite Imagery using Vision Transformers and Nighttime Lights

This repository contains a complete, reproducible pipeline for estimating urban economic intensity proxies from satellite imagery. We combine high-resolution SpaceNet aerial imagery with VIIRS nighttime lights and Vision Transformer features, and apply parameter-efficient LoRA fine-tuning for regional adaptation.

The pipeline has been fully executed end-to-end and all notebooks run without errors.

‚∏ª

Repository Structure

notebooks/
‚îÇ
‚îú‚îÄ‚îÄ 1a_shanghai_pipeline.ipynb      # Shanghai data preparation (independent)
‚îú‚îÄ‚îÄ 1b_vegas_pipeline.ipynb         # Las Vegas data preparation (independent)
‚îú‚îÄ‚îÄ 2_pretraining_tests.ipynb       # Backbone-only feature extraction & regression
‚îú‚îÄ‚îÄ 3_cleanup_format_data.ipynb     # Data merging, cleaning, normalization
‚îú‚îÄ‚îÄ 4a_shanghai_training.ipynb      # LoRA fine-tuning (Shanghai)
‚îú‚îÄ‚îÄ 4b_vegas_training.ipynb         # LoRA fine-tuning (Las Vegas)


‚∏ª

Execution Order (Important)

The notebooks must be run in the following order:

1. Data Preparation (independent)
    ‚Ä¢    1a_shanghai_pipeline.ipynb
    ‚Ä¢    1b_vegas_pipeline.ipynb

2. Backbone-Only Experiments (after 1a & 1b)
    ‚Ä¢    2_pretraining_tests.ipynb

3. Data Cleanup & Formatting (after 2)
    ‚Ä¢    3_cleanup_format_data.ipynb

4. LoRA Fine-Tuning (independent, after 3)
    ‚Ä¢    4a_shanghai_training.ipynb
    ‚Ä¢    4b_vegas_training.ipynb

‚∏ª

Initial Setup

1. Environment Requirements
    ‚Ä¢    Python 3.10 (recommended)
    ‚Ä¢    Python ‚â•3.11 is not supported by several dependencies used here
    ‚Ä¢    PyTorch (Apple Silicon MPS supported)
    ‚Ä¢    torchvision
    ‚Ä¢    transformers
    ‚Ä¢    timm
    ‚Ä¢    numpy, pandas
    ‚Ä¢    scikit-learn
    ‚Ä¢    xgboost
    ‚Ä¢    geopandas
    ‚Ä¢    rasterio
    ‚Ä¢    shapely
    ‚Ä¢    matplotlib, seaborn
    ‚Ä¢    jupyter / jupyterlab
    ‚Ä¢    awscli (for SpaceNet download)

‚ö†Ô∏è GPU is not required, but training is faster with GPU or Apple MPS.

‚∏ª

2. DINOv2 Installation (Required)

DINOv2 is installed directly from the official Facebook Research repository.

Run inside your Python environment:

git clone https://github.com/facebookresearch/dinov2.git
cd dinov2
pip install -e .

xFormers is optional. Warnings about missing xFormers are expected and do not affect correctness.

‚∏ª

Data Sources

3. Satellite Imagery (SpaceNet)

We use SpaceNet-2 imagery:
    ‚Ä¢    AOI 4: Shanghai (4,582 tiles)
    ‚Ä¢    AOI 2: Las Vegas (3,850 tiles)

Data is hosted on public AWS S3 mirrors.

Shanghai data is downloaded automatically inside:
    ‚Ä¢    1a_shanghai_pipeline.ipynb

Las Vegas metadata is handled in the pipeline, but two large directories and a file are best downloaded manually via terminal (recommended to avoid notebook interruptions).

Recommended (Terminal Download for Las Vegas)

aws s3 sync s3://spacenet-dataset/spacenet/SN2_buildings/train/AOI_2_Vegas/PS-RGB/ \
    data/spacenet/AOI_2_Vegas/PS-RGB/ --no-sign-request

aws s3 sync s3://spacenet-dataset/spacenet/SN2_buildings/train/AOI_2_Vegas/geojson_buildings/ \
    data/spacenet/AOI_2_Vegas/geojson_buildings/ --no-sign-request

aws s3 sync s3://spacenet-dataset/spacenet/SN2_buildings/train/AOI_2_Vegas/ \
    data/spacenet/AOI_2_Vegas/ --no-sign-request

hese commands are also included (optionally) inside 1b_vegas_pipeline.ipynb, but terminal execution is more reliable.

All SpaceNet data is stored under:

data/spacenet/

‚∏ª

4. Nighttime Lights (Required Before Running)

‚ö†Ô∏è This step must be done manually before running the notebooks.

We use:
    ‚Ä¢    VIIRS VNL v21
    ‚Ä¢    Year 2021 annual composite

Source
NASA / NOAA Earth Observation Group (EOG)

üîó Direct download:
https://eogdata.mines.edu/nighttime_light/annual/v20/2021/VNL_v2_npp_2021_global_vcmslcfg_c202203152300.average_masked.tif.gz

Homepage:
https://eogdata.mines.edu/products/vnl/

Instructions
    1.    Create a free account on the EOG website
    2.    Download the VIIRS annual composite
    3.    Place the file under:
    
data/night_lights/

The notebooks assume the nighttime lights data already exists locally and do not download it automatically.

‚∏ª

Coordinate Systems

For accurate spatial overlap:
    ‚Ä¢    Las Vegas ‚Üí UTM Zone 11N
    ‚Ä¢    Shanghai ‚Üí UTM Zone 51N

Reprojection, cropping and aggregation are handled inside the notebooks.

‚∏ª

Training Details (Summary)
    ‚Ä¢    Backbone: DINOv2 Vision Transformer (frozen)
    ‚Ä¢    Target: mean nighttime radiance (VIIRS DN)
    ‚Ä¢    Loss: Mean Absolute Error (MAE)
    ‚Ä¢    Optimizer: AdamW
    ‚Ä¢    LoRA fine-tuning:
    ‚Ä¢    Only LoRA adapters + regression head are trainable
    ‚Ä¢    Backbone weights remain frozen
    ‚Ä¢    Region-specific hyperparameters used for Shanghai and Las Vegas
    ‚Ä¢    Training performed on Apple Silicon (MPS backend)

‚∏ª

Reproducibility Checklist
    ‚Ä¢    Public datasets only (SpaceNet, VIIRS)
    ‚Ä¢    Exact data sources documented
    ‚Ä¢    Clear execution order across notebooks
    ‚Ä¢    Random train/validation split (80/20)
    ‚Ä¢    Metrics reported: MAE and R¬≤
    ‚Ä¢    Region-specific preprocessing disclosed
    ‚Ä¢    Fully rerunnable end-to-end pipeline
    ‚Ä¢    Deterministic preprocessing and logging

‚∏ª

Citation

If you use this repository, please cite this codebase and/or the accompanying paper (to be provided)
