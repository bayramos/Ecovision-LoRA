{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6bb8b9-b0b7-4821-94f4-906464cd29b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "\n",
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "PRINT_EVERY = 50\n",
    "\n",
    "# LoRA hyperparams\n",
    "LORA_R = 8\n",
    "LORA_ALPHA = 16\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "MODEL_NAME = \"vit_base_patch14_dinov2\"\n",
    "IMAGE_SIZE = 518\n",
    "\n",
    "# ------------------- LoRA modules -------------------\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, base_linear, r=LORA_R, alpha=LORA_ALPHA, dropout=LORA_DROPOUT):\n",
    "        super().__init__()\n",
    "        self.base = base_linear\n",
    "        in_f, out_f = base_linear.in_features, base_linear.out_features\n",
    "        self.r = r\n",
    "\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(torch.zeros(in_f, r))\n",
    "            self.lora_B = nn.Parameter(torch.zeros(r, out_f))\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "            self.scaling = alpha / r\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "            self.scaling = 1\n",
    "            self.dropout = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_out = self.base(x)\n",
    "        if self.r > 0:\n",
    "            dx = self.dropout(x)\n",
    "            lora_out = (dx @ self.lora_A) @ self.lora_B\n",
    "            return base_out + self.scaling * lora_out\n",
    "        return base_out\n",
    "\n",
    "\n",
    "def replace_modules_by_path(model, substrs=(\"qkv\", \"proj\")):\n",
    "    replaced = []\n",
    "    for full_name, module in list(model.named_modules()):\n",
    "        if any(s in full_name for s in substrs):\n",
    "            parent_name = \".\".join(full_name.split(\".\")[:-1])\n",
    "            leaf = full_name.split(\".\")[-1]\n",
    "\n",
    "            parent = model\n",
    "            if parent_name:\n",
    "                for p in parent_name.split(\".\"):\n",
    "                    parent = getattr(parent, p)\n",
    "\n",
    "            old = getattr(parent, leaf)\n",
    "            if isinstance(old, nn.Linear):\n",
    "                new = LoRALinear(old)\n",
    "                setattr(parent, leaf, new)\n",
    "                replaced.append(full_name)\n",
    "    return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6951e9ac-2107-417d-852e-a84f2e624750",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_col=\"image_path\", target_col=\"target\",\n",
    "                 image_size=IMAGE_SIZE):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.paths = self.df[image_col].tolist()\n",
    "        self.targets = self.df[target_col].astype(float).values\n",
    "\n",
    "        self.tf = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "        ])\n",
    "\n",
    "    def load_image(self, path):\n",
    "        try:\n",
    "            with rasterio.open(path) as src:\n",
    "                arr = src.read([1,2,3])\n",
    "                arr = np.transpose(arr, (1,2,0))\n",
    "                arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "                return Image.fromarray(arr)\n",
    "        except:\n",
    "            return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.load_image(self.paths[idx])\n",
    "        img = self.tf(img)\n",
    "        y = float(self.targets[idx])\n",
    "        return img, torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb7d03ba-8a7f-4fd1-8a71-0a2388e921c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Device:\", DEVICE)\n",
    "print(\"Loading:\", MODEL_NAME)\n",
    "\n",
    "# 1) load to CPU first (important)\n",
    "model = timm.create_model(MODEL_NAME, pretrained=True)\n",
    "\n",
    "# find embedding dimension\n",
    "embed_dim = model.num_features if hasattr(model, \"num_features\") else model.embed_dim\n",
    "print(\"Embedding dim:\", embed_dim)\n",
    "\n",
    "# replace classification head with regression head\n",
    "model.reset_classifier(num_classes=0)\n",
    "model.head = nn.Linear(embed_dim, 1)\n",
    "\n",
    "# freeze base parameters\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Patching LoRA...\")\n",
    "replaced = replace_modules_by_path(model, (\"qkv\",\"proj\"))\n",
    "print(\"LoRA replaced modules:\", len(replaced))\n",
    "\n",
    "# unfreeze LoRA params + head\n",
    "for n, p in model.named_parameters():\n",
    "    if \"lora_\" in n or n.startswith(\"head\"):\n",
    "        p.requires_grad = True\n",
    "\n",
    "# move model to device\n",
    "model = model.to(DEVICE)\n",
    "print(\"Model moved to device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd7c58f8-1a0e-4c08-90f7-03b4e4e22c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = \"../data/output/shanghai_merged_full_clean.csv\"\n",
    "\n",
    "ds = ImgDataset(CSV)\n",
    "N = len(ds)\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx) \n",
    "\n",
    "train_idx = idx[: int(0.8*N)]\n",
    "val_idx   = idx[int(0.8*N):]\n",
    "\n",
    "train_ds = torch.utils.data.Subset(ds, train_idx)\n",
    "val_ds   = torch.utils.data.Subset(ds, val_idx)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=4, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=4, shuffle=False)\n",
    "\n",
    "print(\"Train:\", len(train_ds), \" Val:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32d3bf2b-08e7-4840-9e31-f5e9412d5b76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_reg(m, x):\n",
    "    out = m(x)\n",
    "    if out.ndim == 1:\n",
    "        out = out.unsqueeze(1)\n",
    "    return out\n",
    "\n",
    "criterion = nn.HuberLoss(delta=5.0)\n",
    "\n",
    "trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable, lr=3e-5, weight_decay=1e-6)\n",
    "\n",
    "print(\"Trainable params:\", sum(p.numel() for p in trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a323988-a370-4bc1-b6ac-dffc3970d0cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 - LoRA training\n",
    "\n",
    "from tqdm import tqdm\n",
    "\n",
    "EPOCHS = 30\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "# --- ADD THESE HISTORY LISTS ---\n",
    "history_train_mae = []\n",
    "history_val_mae = []\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total, n = 0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Train ep{ep}/{EPOCHS}\")\n",
    "\n",
    "    for imgs, ys in pbar:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        ys = ys.to(DEVICE).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = forward_reg(model, imgs)\n",
    "        loss = criterion(preds, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item() * imgs.size(0)\n",
    "        n += imgs.size(0)\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_mae = total / n\n",
    "    history_train_mae.append(train_mae)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    total, n = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, ys in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            ys = ys.to(DEVICE).unsqueeze(1)\n",
    "            preds = forward_reg(model, imgs)\n",
    "            total += torch.abs(preds - ys).sum().item()\n",
    "            n += imgs.size(0)\n",
    "\n",
    "    val_mae = total / n\n",
    "    history_val_mae.append(val_mae)\n",
    "\n",
    "    print(f\"Epoch {ep}/{EPOCHS} Train MAE={train_mae:.4f}  Val MAE={val_mae:.4f}\")\n",
    "\n",
    "    # Save checkpoint if best val\n",
    "    if val_mae < best_val:\n",
    "        best_val = val_mae\n",
    "        os.makedirs(\"../models\", exist_ok=True)\n",
    "        torch.save(\n",
    "            {k: v.cpu() for k,v in model.state_dict().items()\n",
    "             if (\"lora_\" in k or k.startswith(\"head\"))},\n",
    "            \"../models/dinov2_shanghai_lora_best.pth\"\n",
    "        )\n",
    "        print(\"Saved new best checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3753f9d4-4477-42bb-b3c8-0b20a1c1f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CELL 7 â€” final evaluation + history logging\n",
    "\n",
    "# ---- Final evaluation on validation set ----\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, ys in val_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        ys = ys.to(DEVICE).unsqueeze(1)\n",
    "        preds = forward_reg(model, imgs)\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(ys.cpu().numpy())\n",
    "\n",
    "preds = np.vstack(all_preds).ravel()\n",
    "targets = np.vstack(all_targets).ravel()\n",
    "\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "final_r2 = r2_score(targets, preds)\n",
    "final_mae = mean_absolute_error(targets, preds)\n",
    "\n",
    "print(\"Final R2:\", final_r2)\n",
    "print(\"Final MAE:\", final_mae)\n",
    "\n",
    "# ---- LOG TRAINING HISTORY TO CSV ----\n",
    "history_path = \"../models/dinov2_shanghai_training_history.csv\"\n",
    "os.makedirs(os.path.dirname(history_path), exist_ok=True)\n",
    "\n",
    "df_hist = pd.DataFrame({\n",
    "    \"epoch\": list(range(1, len(history_train_mae)+1)),\n",
    "    \"train_mae\": history_train_mae,\n",
    "    \"val_mae\": history_val_mae\n",
    "})\n",
    "\n",
    "df_hist.to_csv(history_path, index=False)\n",
    "print(\"Saved training history to:\", history_path)\n",
    "\n",
    "# Optional: save last-epoch LoRA params as well\n",
    "last_path = \"../models/dinov2_shanghai_lora_last.pth\"\n",
    "torch.save(\n",
    "    {k: v.cpu() for k,v in model.state_dict().items()\n",
    "     if (\"lora_\" in k or k.startswith(\"head\"))},\n",
    "    last_path\n",
    ")\n",
    "print(\"Saved final epoch LoRA params to:\", last_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
