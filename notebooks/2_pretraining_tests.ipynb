{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e9c3b922-4b47-432e-859d-eb60799cbf54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full pipeline start. OUTPUT_DIR= ../data/output/cross_city_results\n",
      "\n",
      "-> Running Shanghai-only training\n",
      "\n",
      "=== Combined training on ['../data/output/shanghai_merged_full.csv'] ===\n",
      "Loading: ../data/output/shanghai_merged_full.csv\n",
      " -> shape: (4582, 782)\n",
      "  linear       R² = 0.268750  MAE = 13.926  time=0.1s\n",
      "  random_forest R² = 0.476686  MAE = 10.196  time=144.1s\n",
      "  mlp          R² = 0.469725  MAE = 10.608  time=7.8s\n",
      "  xgboost      R² = 0.408690  MAE = 11.022  time=3.7s\n",
      "\n",
      "-> Running Vegas-only training\n",
      "\n",
      "=== Combined training on ['../data/output/vegas_merged_full.csv'] ===\n",
      "Loading: ../data/output/vegas_merged_full.csv\n",
      " -> shape: (3847, 777)\n",
      "  linear       R² = 0.137553  MAE = 53.344  time=0.1s\n",
      "  random_forest R² = 0.422090  MAE = 31.094  time=132.3s\n",
      "  mlp          R² = 0.352351  MAE = 36.852  time=3.0s\n",
      "  xgboost      R² = 0.383343  MAE = 31.217  time=4.6s\n",
      "\n",
      "=== Cross eval: TRAIN ../data/output/shanghai_merged_full.csv -> TEST ../data/output/vegas_merged_full.csv ===\n",
      "Loading: ../data/output/shanghai_merged_full.csv\n",
      " -> shape: (4582, 782)\n",
      "Loading: ../data/output/vegas_merged_full.csv\n",
      " -> shape: (3847, 777)\n",
      "Aligned feature count = 771\n",
      "  linear       R² = -65.501389  MAE = 626.791  time=0.1s\n",
      "  random_forest R² = -0.010138  MAE = 37.079  time=132.2s\n",
      "  mlp          R² = -0.571619  MAE = 66.853  time=9.3s\n",
      "  xgboost      R² = -0.079972  MAE = 39.626  time=3.9s\n",
      "\n",
      "=== Cross eval: TRAIN ../data/output/vegas_merged_full.csv -> TEST ../data/output/shanghai_merged_full.csv ===\n",
      "Loading: ../data/output/vegas_merged_full.csv\n",
      " -> shape: (3847, 777)\n",
      "Loading: ../data/output/shanghai_merged_full.csv\n",
      " -> shape: (4582, 782)\n",
      "Aligned feature count = 771\n",
      "  linear       R² = -109.997116  MAE = 216.342  time=0.1s\n",
      "  random_forest R² = -29.076919  MAE = 111.024  time=141.7s\n",
      "  mlp          R² = -87.387871  MAE = 183.394  time=3.8s\n",
      "  xgboost      R² = -35.436546  MAE = 126.304  time=4.7s\n",
      "\n",
      "=== Combined training on ['../data/output/shanghai_merged_full.csv', '../data/output/vegas_merged_full.csv'] ===\n",
      "Loading: ../data/output/shanghai_merged_full.csv\n",
      " -> shape: (4582, 782)\n",
      "Loading: ../data/output/vegas_merged_full.csv\n",
      " -> shape: (3847, 777)\n",
      "  linear       R² = 0.247575  MAE = 32.023  time=0.1s\n",
      "  random_forest R² = 0.444402  MAE = 21.134  time=262.4s\n",
      "  mlp          R² = 0.473260  MAE = 21.312  time=3.6s\n",
      "  xgboost      R² = 0.440274  MAE = 22.117  time=4.8s\n",
      "All done. Total time: 867.0 s\n"
     ]
    }
   ],
   "source": [
    "# LoRA Fine Tuning DINOv2-B \n",
    "\n",
    "#!/usr/bin/env python3\n",
    "\"\"\"\n",
    "Full final pipeline (clean, align, train, cross-eval, combined-eval).\n",
    "\n",
    "Drop-in script. Edit file paths below and run:\n",
    "    python full_pipeline_final.py\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import joblib\n",
    "import traceback\n",
    "from typing import Tuple, List, Dict\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.metrics import r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Optional: XGBoost\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    XGBOOST_AVAILABLE = True\n",
    "except Exception:\n",
    "    XGBOOST_AVAILABLE = False\n",
    "\n",
    "# ----------------------------\n",
    "# CONFIG - edit paths / settings\n",
    "# ----------------------------\n",
    "VEG_CSV = \"../data/output/vegas_merged_full.csv\"\n",
    "SH_CSV = \"../data/output/shanghai_merged_full.csv\"\n",
    "\n",
    "OUTPUT_DIR = \"../data/output/cross_city_results\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "TARGET_COL = \"lights_sum\"        # change if your target column name differs\n",
    "MIN_TEST_FRAC = 0.05\n",
    "TEST_FRAC = 0.20                 # used for combined-run (80/20)\n",
    "VAL_FRAC = 0.10                  # optional validation split fraction when training on one city and testing on another\n",
    "RANDOM_STATE = 42\n",
    "\n",
    "# Columns which if present should be dropped (IDs, paths, coordinates, leakage)\n",
    "COMMON_DROP_SUBSTR = [\n",
    "    \"tile_file\", \"tile_crs\", \"geojson_file\", \"image_path\", \"tile_num\",\n",
    "    \"minx\", \"miny\", \"maxx\", \"maxy\", \"tile\", \"tile_id\", \"city\"\n",
    "]\n",
    "# Explicit leakage proxies to drop (if present)\n",
    "LEAKAGE_COLS = [\"lights_mean\", \"lights_nonzero\", \"target\"]  # lights_mean is a near-duplicate of lights_sum\n",
    "# ----------------------------\n",
    "\n",
    "def read_df(path: str) -> pd.DataFrame:\n",
    "    print(\"Loading:\", path)\n",
    "    df = pd.read_csv(path, low_memory=False)\n",
    "    print(\" -> shape:\", df.shape)\n",
    "    return df\n",
    "\n",
    "def autodrop_cols(df: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
    "    \"\"\"Drop non-numeric / id-like / coordinate / leakage columns automatically.\n",
    "       Returns cleaned df and list of dropped columns.\n",
    "    \"\"\"\n",
    "    df2 = df.copy()\n",
    "    drop_cols = []\n",
    "\n",
    "    # 1) drop exact leakage columns if present\n",
    "    for c in LEAKAGE_COLS:\n",
    "        if c in df2.columns:\n",
    "            drop_cols.append(c)\n",
    "            df2 = df2.drop(columns=c)\n",
    "\n",
    "    # 2) drop columns containing substrings (IDs, paths, coords)\n",
    "    for s in COMMON_DROP_SUBSTR:\n",
    "        for c in list(df2.columns):\n",
    "            if s in c:\n",
    "                if c not in drop_cols:\n",
    "                    drop_cols.append(c)\n",
    "                    df2 = df2.drop(columns=c)\n",
    "\n",
    "    # 3) drop explicitly non-numeric columns (object / text) except target\n",
    "    for c in list(df2.select_dtypes(include=[\"object\", \"string\"]).columns):\n",
    "        if c not in [TARGET_COL]:\n",
    "            drop_cols.append(c)\n",
    "            df2 = df2.drop(columns=c)\n",
    "\n",
    "    # 4) drop columns that are constant (zero variance)\n",
    "    const_cols = [c for c in df2.columns if df2[c].nunique(dropna=False) <= 1 and c != TARGET_COL]\n",
    "    for c in const_cols:\n",
    "        drop_cols.append(c)\n",
    "        df2 = df2.drop(columns=c)\n",
    "\n",
    "    return df2, sorted(set(drop_cols))\n",
    "\n",
    "def ensure_target(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Ensure the target column exists. If CSV has 'target' or other, map to TARGET_COL.\"\"\"\n",
    "    if TARGET_COL in df.columns:\n",
    "        return df\n",
    "    # try common alternatives\n",
    "    for alt in [\"target\", \"lights_sum\", \"sum_lights\", \"nl_sum\"]:\n",
    "        if alt in df.columns:\n",
    "            df = df.rename(columns={alt: TARGET_COL})\n",
    "            return df\n",
    "    raise ValueError(f\"Target column '{TARGET_COL}' not found in dataframe columns: {df.columns.tolist()}\")\n",
    "\n",
    "def prepare_xy(df: pd.DataFrame, feat_list: List[str]=None) -> Tuple[np.ndarray, np.ndarray, List[str]]:\n",
    "    \"\"\"Return numeric X, y and feature names list after dropping leakage/IDs and ensuring no text columns remain.\"\"\"\n",
    "    df = ensure_target(df.copy())\n",
    "    df_clean, dropped = autodrop_cols(df)\n",
    "    # keep only numeric cols (except target)\n",
    "    numeric_cols = [c for c in df_clean.columns if (np.issubdtype(df_clean[c].dtype, np.number) or c == TARGET_COL)]\n",
    "    df_clean = df_clean[numeric_cols]\n",
    "    # reorder with features then target\n",
    "    feature_cols = [c for c in df_clean.columns if c != TARGET_COL]\n",
    "    if feat_list is not None:\n",
    "        # if user provided reference features, intersect and preserve order of feat_list\n",
    "        feature_cols = [c for c in feat_list if c in df_clean.columns]\n",
    "    X = df_clean[feature_cols].values.astype(np.float32)\n",
    "    y = df_clean[TARGET_COL].values.astype(np.float32)\n",
    "    return X, y, feature_cols\n",
    "\n",
    "def align_feature_sets(df_train: pd.DataFrame, df_test: pd.DataFrame) -> Tuple[pd.DataFrame, pd.DataFrame, List[str]]:\n",
    "    \"\"\"Return two dataframes with the same numeric feature columns (intersect).\n",
    "       Drops leakage columns and non-numeric columns first.\n",
    "    \"\"\"\n",
    "    # ensure targets present\n",
    "    df_train = ensure_target(df_train.copy())\n",
    "    df_test = ensure_target(df_test.copy())\n",
    "\n",
    "    # autodrop text/id columns but keep target\n",
    "    t1, d1 = autodrop_cols(df_train)\n",
    "    t2, d2 = autodrop_cols(df_test)\n",
    "\n",
    "    # keep numeric-only (and target)\n",
    "    numeric1 = [c for c in t1.columns if (np.issubdtype(t1[c].dtype, np.number) or c == TARGET_COL)]\n",
    "    numeric2 = [c for c in t2.columns if (np.issubdtype(t2[c].dtype, np.number) or c == TARGET_COL)]\n",
    "\n",
    "    # intersect features (exclude target)\n",
    "    feats1 = set([c for c in numeric1 if c != TARGET_COL])\n",
    "    feats2 = set([c for c in numeric2 if c != TARGET_COL])\n",
    "    common_feats = sorted(list(feats1.intersection(feats2)))\n",
    "    if len(common_feats) == 0:\n",
    "        raise ValueError(\"No aligned numeric features between train/test CSVs. Check column names (f1..fN) and cleaning.\")\n",
    "    # build new dataframes keeping only common_feats and target\n",
    "    df_train_a = t1[common_feats + [TARGET_COL]].copy()\n",
    "    df_test_a  = t2[common_feats + [TARGET_COL]].copy()\n",
    "    return df_train_a, df_test_a, common_feats\n",
    "\n",
    "def fit_impute_scale(X_train: np.ndarray) -> Tuple[SimpleImputer, StandardScaler, np.ndarray]:\n",
    "    imputer = SimpleImputer(strategy=\"median\")\n",
    "    scaler = StandardScaler()\n",
    "    X_imp = imputer.fit_transform(X_train)\n",
    "    X_scaled = scaler.fit_transform(X_imp)\n",
    "    return imputer, scaler, X_scaled\n",
    "\n",
    "def transform_with_imputer_scaler(X: np.ndarray, imputer: SimpleImputer, scaler: StandardScaler) -> np.ndarray:\n",
    "    X_imp = imputer.transform(X)\n",
    "    X_scaled = scaler.transform(X_imp)\n",
    "    return X_scaled\n",
    "\n",
    "def train_and_eval_models(Xtr: np.ndarray, ytr: np.ndarray, Xte: np.ndarray, yte: np.ndarray,\n",
    "                          outprefix: str) -> Dict[str, Dict]:\n",
    "    \"\"\"Train four models and evaluate. Returns dict of metrics and saved filenames.\"\"\"\n",
    "    results = {}\n",
    "    os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "    models = {\n",
    "        \"linear\": LinearRegression(),\n",
    "        \"random_forest\": RandomForestRegressor(n_estimators=250, n_jobs=4, random_state=RANDOM_STATE),\n",
    "        \"mlp\": MLPRegressor(hidden_layer_sizes=(512,256), max_iter=200, random_state=RANDOM_STATE)\n",
    "    }\n",
    "    if XGBOOST_AVAILABLE:\n",
    "        models[\"xgboost\"] = xgb.XGBRegressor(n_estimators=200, tree_method=\"hist\", n_jobs=4, random_state=RANDOM_STATE)\n",
    "\n",
    "    for name, model in models.items():\n",
    "        try:\n",
    "            t0 = time.time()\n",
    "            model.fit(Xtr, ytr)\n",
    "            preds = model.predict(Xte)\n",
    "            r2 = r2_score(yte, preds)\n",
    "            mae = mean_absolute_error(yte, preds)\n",
    "            elapsed = time.time() - t0\n",
    "\n",
    "            # save model\n",
    "            model_fname = os.path.join(OUTPUT_DIR, f\"{outprefix}__{name}_model.joblib\")\n",
    "            joblib.dump(model, model_fname)\n",
    "\n",
    "            # save preds CSV\n",
    "            preds_df = pd.DataFrame({\"y_true\": yte, \"y_pred\": preds})\n",
    "            preds_csv = os.path.join(OUTPUT_DIR, f\"{outprefix}__{name}_preds.csv\")\n",
    "            preds_df.to_csv(preds_csv, index=False)\n",
    "\n",
    "            results[name] = {\n",
    "                \"r2\": float(r2),\n",
    "                \"mae\": float(mae),\n",
    "                \"time_s\": elapsed,\n",
    "                \"model_file\": model_fname,\n",
    "                \"preds_csv\": preds_csv\n",
    "            }\n",
    "            print(f\"  {name:<12} R² = {r2:.6f}  MAE = {mae:.3f}  time={elapsed:.1f}s\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error training {name}: {repr(e)}\")\n",
    "            traceback.print_exc()\n",
    "    return results\n",
    "\n",
    "def run_city_pair(train_csv: str, test_csv: str, prefix: str, val_frac: float=VAL_FRAC):\n",
    "    \"\"\"Train on train_csv and evaluate on test_csv. We will:\n",
    "       - align features\n",
    "       - impute and scale using training set\n",
    "       - optionally keep small val set from training for model selection (not used here)\n",
    "    \"\"\"\n",
    "    print(\"\\n=== Cross eval: TRAIN\", train_csv, \"-> TEST\", test_csv, \"===\")\n",
    "    df_tr = read_df(train_csv)\n",
    "    df_te = read_df(test_csv)\n",
    "\n",
    "    # Align features & ensure target\n",
    "    df_tr_a, df_te_a, feat_list = align_feature_sets(df_tr, df_te)\n",
    "    print(\"Aligned feature count =\", len(feat_list))\n",
    "\n",
    "    # optionally split off a val set from training (not required for these models)\n",
    "    if 0.0 < val_frac < 0.5:\n",
    "        Xtr_full, ytr_full, _ = prepare_xy(df_tr_a)\n",
    "        Xtr, Xval, ytr, yval = train_test_split(Xtr_full, ytr_full, test_size=val_frac, random_state=RANDOM_STATE)\n",
    "    else:\n",
    "        Xtr, ytr, _ = prepare_xy(df_tr_a)\n",
    "        Xval, yval = None, None\n",
    "\n",
    "    Xte, yte, _ = prepare_xy(df_te_a)\n",
    "\n",
    "    # Fit imputer+scaler on training\n",
    "    imputer, scaler, Xtr_scaled = fit_impute_scale(Xtr)\n",
    "    Xte_scaled = transform_with_imputer_scaler(Xte, imputer, scaler)\n",
    "\n",
    "    # Save preprocessing\n",
    "    prep_basename = os.path.join(OUTPUT_DIR, f\"{prefix}__prep.joblib\")\n",
    "    joblib.dump({\"imputer\": imputer, \"scaler\": scaler, \"features\": feat_list}, prep_basename)\n",
    "\n",
    "    # Run models\n",
    "    results = train_and_eval_models(Xtr_scaled, ytr, Xte_scaled, yte, outprefix=prefix)\n",
    "    # save summary\n",
    "    with open(os.path.join(OUTPUT_DIR, f\"{prefix}__summary.json\"), \"w\") as fh:\n",
    "        json.dump({\"features\": feat_list, \"results\": results}, fh, indent=2)\n",
    "    return results\n",
    "\n",
    "def run_combined_benchmark(csv_list: List[str], outprefix: str, test_frac: float=TEST_FRAC):\n",
    "    \"\"\"Combine CSVs, then run 80/20 train/test on combined data.\"\"\"\n",
    "    print(\"\\n=== Combined training on\", csv_list, \"===\")\n",
    "    dfs = [read_df(p) for p in csv_list]\n",
    "    # align across all csvs by intersection of features\n",
    "    # start with first\n",
    "    base = dfs[0]\n",
    "    for other in dfs[1:]:\n",
    "        base, other, feats = align_feature_sets(base, other)\n",
    "        base = pd.concat([base, other], ignore_index=True)\n",
    "    df_comb = pd.concat(dfs, ignore_index=True)\n",
    "    df_comb = ensure_target(df_comb)\n",
    "    # drop leaks/ids\n",
    "    df_comb_clean, dropped = autodrop_cols(df_comb)\n",
    "    # ensure numeric only + target\n",
    "    numeric_cols = [c for c in df_comb_clean.columns if (np.issubdtype(df_comb_clean[c].dtype, np.number) or c == TARGET_COL)]\n",
    "    df_comb_clean = df_comb_clean[numeric_cols]\n",
    "\n",
    "    # split 80/20\n",
    "    if not (0.0 < test_frac < 1.0):\n",
    "        test_frac = max(MIN_TEST_FRAC, TEST_FRAC)\n",
    "    X_all = df_comb_clean[[c for c in df_comb_clean.columns if c != TARGET_COL]].values.astype(np.float32)\n",
    "    y_all = df_comb_clean[TARGET_COL].values.astype(np.float32)\n",
    "    Xtr, Xte, ytr, yte = train_test_split(X_all, y_all, test_size=test_frac, random_state=RANDOM_STATE)\n",
    "\n",
    "    # imputer + scaler\n",
    "    imputer, scaler, Xtr_scaled = fit_impute_scale(Xtr)\n",
    "    Xte_scaled = transform_with_imputer_scaler(Xte, imputer, scaler)\n",
    "    joblib.dump({\"imputer\": imputer, \"scaler\": scaler, \"features\": df_comb_clean.columns.tolist()}, os.path.join(OUTPUT_DIR, f\"{outprefix}__prep.joblib\"))\n",
    "\n",
    "    results = train_and_eval_models(Xtr_scaled, ytr, Xte_scaled, yte, outprefix=outprefix)\n",
    "    with open(os.path.join(OUTPUT_DIR, f\"{outprefix}__summary.json\"), \"w\") as fh:\n",
    "        json.dump({\"results\": results}, fh, indent=2)\n",
    "    return results\n",
    "\n",
    "# ----------------------------\n",
    "# Main runner\n",
    "# ----------------------------\n",
    "def main():\n",
    "    tstart = time.time()\n",
    "    print(\"Full pipeline start. OUTPUT_DIR=\", OUTPUT_DIR)\n",
    "    # Basic validations\n",
    "    for path in [VEG_CSV, SH_CSV]:\n",
    "        if not os.path.exists(path):\n",
    "            raise FileNotFoundError(f\"CSV not found: {path}. Edit paths at top of script and try again.\")\n",
    "\n",
    "    # 1) Shanghai-only\n",
    "    try:\n",
    "        print(\"\\n-> Running Shanghai-only training\")\n",
    "        run_combined_benchmark([SH_CSV], outprefix=\"shanghai_only\")\n",
    "    except Exception as e:\n",
    "        print(\"Shanghai-only failed:\", repr(e))\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 2) Vegas-only\n",
    "    try:\n",
    "        print(\"\\n-> Running Vegas-only training\")\n",
    "        run_combined_benchmark([VEG_CSV], outprefix=\"vegas_only\")\n",
    "    except Exception as e:\n",
    "        print(\"Vegas-only failed:\", repr(e))\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 3) Cross-city: Shanghai -> Vegas\n",
    "    try:\n",
    "        run_city_pair(SH_CSV, VEG_CSV, prefix=\"shanghai_TO_vegas\")\n",
    "    except Exception as e:\n",
    "        print(\"Cross sh->vg failed:\", repr(e))\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 4) Cross-city: Vegas -> Shanghai\n",
    "    try:\n",
    "        run_city_pair(VEG_CSV, SH_CSV, prefix=\"vegas_TO_shanghai\")\n",
    "    except Exception as e:\n",
    "        print(\"Cross vg->sh failed:\", repr(e))\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # 5) Combined training (80/20)\n",
    "    try:\n",
    "        run_combined_benchmark([SH_CSV, VEG_CSV], outprefix=\"combined_sh_vg\")\n",
    "    except Exception as e:\n",
    "        print(\"Combined training failed:\", repr(e))\n",
    "        traceback.print_exc()\n",
    "\n",
    "    print(\"All done. Total time: %.1f s\" % (time.time() - tstart))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
