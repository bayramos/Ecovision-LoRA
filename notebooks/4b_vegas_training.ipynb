{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c99cc6bc-ef8b-4945-adc3-d78fe5b6c2cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vegas Training\n",
    "\n",
    "# import libraries\n",
    "import os, time, math\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as T\n",
    "import timm\n",
    "import rasterio\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import r2_score, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0c8d254-54c2-48fc-b15d-68ebbbe6ced0",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "PRINT_EVERY = 50\n",
    "\n",
    "# LoRA hyperparams\n",
    "LORA_R = 32\n",
    "LORA_ALPHA = 64\n",
    "LORA_DROPOUT = 0.05\n",
    "\n",
    "MODEL_NAME = \"vit_base_patch14_dinov2\"\n",
    "IMAGE_SIZE = 518\n",
    "\n",
    "# ------------------- LoRA modules -------------------\n",
    "class LoRALinear(nn.Module):\n",
    "    def __init__(self, base_linear, r=LORA_R, alpha=LORA_ALPHA, dropout=LORA_DROPOUT):\n",
    "        super().__init__()\n",
    "        self.base = base_linear\n",
    "        in_f, out_f = base_linear.in_features, base_linear.out_features\n",
    "        self.r = r\n",
    "\n",
    "        if r > 0:\n",
    "            self.lora_A = nn.Parameter(torch.zeros(in_f, r))\n",
    "            self.lora_B = nn.Parameter(torch.zeros(r, out_f))\n",
    "            nn.init.kaiming_uniform_(self.lora_A, a=math.sqrt(5))\n",
    "            nn.init.zeros_(self.lora_B)\n",
    "            self.scaling = alpha / r\n",
    "            self.dropout = nn.Dropout(dropout)\n",
    "        else:\n",
    "            self.lora_A = None\n",
    "            self.lora_B = None\n",
    "            self.scaling = 1\n",
    "            self.dropout = nn.Identity()\n",
    "\n",
    "    def forward(self, x):\n",
    "        base_out = self.base(x)\n",
    "        if self.r > 0:\n",
    "            dx = self.dropout(x)\n",
    "            lora_out = (dx @ self.lora_A) @ self.lora_B\n",
    "            return base_out + self.scaling * lora_out\n",
    "        return base_out\n",
    "\n",
    "\n",
    "def replace_modules_by_path(model, substrs=(\"qkv\", \"proj\")):\n",
    "    replaced = []\n",
    "    for full_name, module in list(model.named_modules()):\n",
    "        if any(s in full_name for s in substrs):\n",
    "            parent_name = \".\".join(full_name.split(\".\")[:-1])\n",
    "            leaf = full_name.split(\".\")[-1]\n",
    "\n",
    "            parent = model\n",
    "            if parent_name:\n",
    "                for p in parent_name.split(\".\"):\n",
    "                    parent = getattr(parent, p)\n",
    "\n",
    "            old = getattr(parent, leaf)\n",
    "            if isinstance(old, nn.Linear):\n",
    "                new = LoRALinear(old)\n",
    "                setattr(parent, leaf, new)\n",
    "                replaced.append(full_name)\n",
    "    return replaced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4777bdd-75dd-48d2-8a67-16fc0afcaea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImgDataset(Dataset):\n",
    "    def __init__(self, csv_path, image_col=\"image_path\", target_col=\"target\",\n",
    "                 image_size=IMAGE_SIZE):\n",
    "        self.df = pd.read_csv(csv_path)\n",
    "        self.paths = self.df[image_col].tolist()\n",
    "        self.targets = self.df[target_col].astype(float).values\n",
    "\n",
    "        self.tf = T.Compose([\n",
    "            T.Resize((image_size, image_size)),\n",
    "            T.ToTensor(),\n",
    "            T.Normalize([0.485,0.456,0.406], [0.229,0.224,0.225])\n",
    "        ])\n",
    "\n",
    "    def load_image(self, path):\n",
    "        try:\n",
    "            with rasterio.open(path) as src:\n",
    "                arr = src.read([1,2,3])\n",
    "                arr = np.transpose(arr, (1,2,0))\n",
    "                arr = np.clip(arr, 0, 255).astype(np.uint8)\n",
    "                return Image.fromarray(arr)\n",
    "        except:\n",
    "            return Image.open(path).convert(\"RGB\")\n",
    "\n",
    "    def __len__(self): return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img = self.load_image(self.paths[idx])\n",
    "        img = self.tf(img)\n",
    "        y = float(self.targets[idx])\n",
    "        return img, torch.tensor(y, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c971cf86-4152-420c-a6c4-25a98454fd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Device:\", DEVICE)\n",
    "print(\"Loading:\", MODEL_NAME)\n",
    "\n",
    "# load to CPU first \n",
    "model = timm.create_model(MODEL_NAME, pretrained=True)\n",
    "\n",
    "# find embedding dimension\n",
    "embed_dim = model.num_features if hasattr(model, \"num_features\") else model.embed_dim\n",
    "print(\"Embedding dim:\", embed_dim)\n",
    "\n",
    "# replace classification head with regression head\n",
    "model.reset_classifier(num_classes=0)\n",
    "model.head = nn.Linear(embed_dim, 1)\n",
    "\n",
    "# freeze base parameters\n",
    "for p in model.parameters():\n",
    "    p.requires_grad = False\n",
    "\n",
    "print(\"Patching LoRA...\")\n",
    "replaced = replace_modules_by_path(model, (\"qkv\",\"proj\"))\n",
    "print(\"LoRA replaced modules:\", len(replaced))\n",
    "\n",
    "# unfreeze LoRA params + head\n",
    "for n, p in model.named_parameters():\n",
    "    if \"lora_\" in n or n.startswith(\"head\"):\n",
    "        p.requires_grad = True\n",
    "\n",
    "# move model to device\n",
    "model = model.to(DEVICE)\n",
    "print(\"Model moved to device.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e24e9caf-0321-4fcf-9f90-067cdda0e734",
   "metadata": {},
   "outputs": [],
   "source": [
    "CSV = \"../data/output/vegas_merged_full_clean.csv\"\n",
    "\n",
    "ds = ImgDataset(CSV)\n",
    "N = len(ds)\n",
    "\n",
    "# === Compute training mean/std for normalization ===\n",
    "raw_targets = ds.targets\n",
    "t_mean = raw_targets.mean()\n",
    "t_std  = raw_targets.std()\n",
    "\n",
    "print(\"Target mean:\", t_mean, \" Target std:\", t_std)\n",
    "\n",
    "# Store normalized targets into dataset\n",
    "ds.targets = (raw_targets - t_mean) / t_std\n",
    "\n",
    "idx = np.arange(N)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "train_idx = idx[: int(0.8*N)]\n",
    "val_idx   = idx[int(0.8*N):]\n",
    "\n",
    "train_ds = torch.utils.data.Subset(ds, train_idx)\n",
    "val_ds   = torch.utils.data.Subset(ds, val_idx)\n",
    "\n",
    "# === Updated batch size ===\n",
    "train_loader = DataLoader(train_ds, batch_size=8, shuffle=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=8, shuffle=False)\n",
    "\n",
    "print(\"Train:\", len(train_ds), \" Val:\", len(val_ds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47488af5-e172-47a9-8e14-752ee80d0aa9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward_reg(m, x):\n",
    "    out = m(x)\n",
    "    if out.ndim == 1:\n",
    "        out = out.unsqueeze(1)\n",
    "    return out\n",
    "\n",
    "criterion = nn.L1Loss()       # === MAE loss ===\n",
    "\n",
    "trainable = [p for p in model.parameters() if p.requires_grad]\n",
    "optimizer = torch.optim.AdamW(trainable, lr=3e-5, weight_decay=1e-6)\n",
    "\n",
    "print(\"Trainable params:\", sum(p.numel() for p in trainable))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52724b38-6d3e-4575-8289-86446cbd5f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# LoRA training\n",
    "\n",
    "EPOCHS = 50\n",
    "best_val = float(\"inf\")\n",
    "\n",
    "# ADD THESE HISTORY LISTS\n",
    "history_train_mae = []\n",
    "history_val_mae = []\n",
    "\n",
    "for ep in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total, n = 0, 0\n",
    "    pbar = tqdm(train_loader, desc=f\"Train ep{ep}/{EPOCHS}\")\n",
    "\n",
    "    for imgs, ys in pbar:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        ys = ys.to(DEVICE).unsqueeze(1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        preds = forward_reg(model, imgs)\n",
    "        loss = criterion(preds, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total += loss.item() * imgs.size(0)\n",
    "        n += imgs.size(0)\n",
    "        pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "    train_mae = total / n\n",
    "    history_train_mae.append(train_mae)\n",
    "\n",
    "    # --- Validation ---\n",
    "    model.eval()\n",
    "    total, n = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for imgs, ys in val_loader:\n",
    "            imgs = imgs.to(DEVICE)\n",
    "            ys = ys.to(DEVICE).unsqueeze(1)\n",
    "            preds = forward_reg(model, imgs)\n",
    "            total += torch.abs(preds - ys).sum().item()\n",
    "            n += imgs.size(0)\n",
    "\n",
    "    val_mae = total / n\n",
    "    history_val_mae.append(val_mae)\n",
    "\n",
    "    print(f\"Epoch {ep}/{EPOCHS} Train MAE={train_mae:.4f}  Val MAE={val_mae:.4f}\")\n",
    "\n",
    "    # Save checkpoint if best val\n",
    "    if val_mae < best_val:\n",
    "        best_val = val_mae\n",
    "        os.makedirs(\"../models\", exist_ok=True)\n",
    "        torch.save(\n",
    "            {k: v.cpu() for k,v in model.state_dict().items()\n",
    "             if (\"lora_\" in k or k.startswith(\"head\"))},\n",
    "            \"../models/dinov2_vegas_lora_best.pth\"\n",
    "        )\n",
    "        print(\"Saved new best checkpoint\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c708108-32d6-4697-b925-cbe676fa4f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# final evaluation and history logging\n",
    "\n",
    "# Load BEST LoRA checkpoint before evaluation\n",
    "best_ckpt_path = \"../models/dinov2_vegas_lora_best.pth\"\n",
    "\n",
    "state = torch.load(best_ckpt_path, map_location=DEVICE)\n",
    "model.load_state_dict(state, strict=False)\n",
    "model.eval()\n",
    "\n",
    "print(\"Loaded best validation checkpoint:\", best_ckpt_path)\n",
    "\n",
    "# Final evaluation\n",
    "all_preds, all_targets = [], []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for imgs, ys in val_loader:\n",
    "        imgs = imgs.to(DEVICE)\n",
    "        ys = ys.to(DEVICE).unsqueeze(1)\n",
    "        preds = forward_reg(model, imgs)\n",
    "\n",
    "        all_preds.append(preds.cpu().numpy())\n",
    "        all_targets.append(ys.cpu().numpy())\n",
    "\n",
    "preds = np.vstack(all_preds).ravel()\n",
    "targets = np.vstack(all_targets).ravel()\n",
    "\n",
    "# INVERSE NORMALIZE predictions\n",
    "preds = preds * t_std + t_mean\n",
    "targets = targets * t_std + t_mean\n",
    "\n",
    "final_r2 = r2_score(targets, preds)\n",
    "final_mae = mean_absolute_error(targets, preds)\n",
    "\n",
    "print(\"Final R2 (best model):\", final_r2)\n",
    "print(\"Final MAE (best model):\", final_mae)\n",
    "\n",
    "# Save training history\n",
    "history_path = \"../models/dinov2_vegas_training_history.csv\"\n",
    "df_hist = pd.DataFrame({\n",
    "    \"epoch\": list(range(1, len(history_train_mae) + 1)),\n",
    "    \"train_mae\": history_train_mae,\n",
    "    \"val_mae\": history_val_mae\n",
    "})\n",
    "df_hist.to_csv(history_path, index=False)\n",
    "print(\"Saved training history to:\", history_path)\n",
    "\n",
    "# Save last epoch LoRA separately (for reference)\n",
    "last_path = \"../models/dinov2_vegas_lora_last.pth\"\n",
    "torch.save(\n",
    "    {k: v.cpu() for k, v in model.state_dict().items()\n",
    "     if (\"lora_\" in k or k.startswith(\"head\"))},\n",
    "    last_path\n",
    ")\n",
    "print(\"Saved final epoch LoRA params to:\", last_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
